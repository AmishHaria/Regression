{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Correlation Matrix (Dont Delete)\n",
    "#Values between 0 and 0.3 (0 and -0.3) indicate a weak positive (negative) linear relationship via a shaky linear rule.\n",
    "#Values between 0.3 and 0.7 (-0.3 and -0.7) indicate a moderate positive (negative) linear relationship via a fuzzy-firm linear rule.\n",
    "#Values between 0.7 and 1.0 (-0.7 and -1.0) indicate a strong positive (negative) linear relationship via a firm linear rule.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('https://www.dropbox.com/s/tzfp8mfdx1ekrnl/Mattcorrmatrix.csv?raw=1')\n",
    "corr = df.corr(method='pearson') # Can also select spearman\n",
    "\n",
    "# Setup\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# vmin and vmax control the range of the colormap\n",
    "sns.heatmap(corr, cmap='RdBu', annot=True, fmt='.2f',\n",
    "           vmin=-1, vmax=1)\n",
    "\n",
    "plt.title(\"Correlations Between Variables\")\n",
    "\n",
    "# Add tight_layout to ensure the labels don't get cut off\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.clustermap(corr, method='ward', cmap='RdBu', annot=True,\n",
    "               vmin=-1, vmax=1, figsize=(14,12))\n",
    "\n",
    "plt.title(\"Correlations Between Variables\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('test.pdf', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print (corr)\n",
    "export_csv = corr.to_csv ('export_dataframe.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Correlation Matrix ALGO (DONT DELETE)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dfmaster = pd.read_excel('https://www.dropbox.com/s/1ldef8p1patajib/pes.xlsx?raw=1')\n",
    "\n",
    "dfoutput = pd.DataFrame(columns=['Output'])\n",
    "\n",
    "for n in range(0, len(dfmaster.columns)):\n",
    "\n",
    "    df1 = dfmaster.iloc[: , n]\n",
    "    df2 = dfmaster.drop(dfmaster.columns[n],axis=1)\n",
    "\n",
    "    dfaverage = df2.mean(axis=1)\n",
    "\n",
    "    dfnew = pd.concat([df1, dfaverage], axis=1)\n",
    "\n",
    "    corr = dfnew.corr()\n",
    "\n",
    "    corrnew = corr.iloc[0, 1]\n",
    "    \n",
    "    print(corrnew)\n",
    "    \n",
    "    dfoutput.loc[n] = corrnew\n",
    "\n",
    "export_csv = dfoutput.to_csv ('export_dataframe.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Improved Regression y variable at last column\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print('Data Preview')\n",
    "print(df.head(3))\n",
    "print(\" \")\n",
    "print(\"Data Info\")\n",
    "print(df.info())\n",
    "print(\" \")\n",
    "print(\"Data Stats\")\n",
    "dfdescribe = df.describe()\n",
    "print(dfdescribe)\n",
    "columns = df.columns\n",
    "xcolumns = np.delete(columns, -1)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Data Plots\")\n",
    "sns.pairplot(df)\n",
    "plt.savefig('Data_plots.png', dpi=500)\n",
    "plt.show()\n",
    "print(\" \")\n",
    "print(\"Corr Heatmap\")\n",
    "sns.heatmap(df.corr(),cmap = 'coolwarm', annot = True)\n",
    "plt.savefig('Corr_heatmap.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "X = df[xcolumns]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "print('FORCE THROUGH 0,0? Type yes if so')\n",
    "f00 = input()\n",
    "\n",
    "if f00 == 'yes' or f00 == 'Yes' or f00 == 'y' or f00 == 'Y' or f00 == 'YES':\n",
    "    z = False\n",
    "else:\n",
    "    z = True\n",
    "\n",
    "\n",
    "lm = LinearRegression(fit_intercept=z)\n",
    "\n",
    "model = lm.fit(X,y)\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "r_sq = model.score(X, y)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y,predictions))\n",
    "\n",
    "cdf = pd.DataFrame(np.append(lm.coef_, [lm.intercept_, r_sq, rmse]),np.append(X.columns, ['Intercept','R^2', 'RMSE']),columns = ['Coeff'])\n",
    "\n",
    "print(cdf.round(4))\n",
    "\n",
    "plt.scatter(y,predictions)\n",
    "plt.savefig('Predictions_vs_Actual.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "sns.distplot((y-predictions))\n",
    "plt.savefig('Residual_dist.png', dpi=500)\n",
    "plt.show()\n",
    "\n",
    "with pd.ExcelWriter('regression_results.xlsx') as writer:\n",
    "    cdf.to_excel(writer, sheet_name='Regression Coefs')\n",
    "    dfdescribe.to_excel(writer, sheet_name='Variable Stats')\n",
    "    df.to_excel(writer, sheet_name='Original Data')\n",
    "\n",
    "print('Press any key to close')\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Plotting Variables and Linear Regression (Dont Delete)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_excel('https://www.dropbox.com/s/oqtubl4vetqf681/plot.xlsx?raw=1')\n",
    "pd.set_option('max_rows', 5) #limits row output\n",
    "print (df)\n",
    "\n",
    "\n",
    "Var1 = df.iloc[:,0]\n",
    "Var2 = df.iloc[:,1]\n",
    "\n",
    "\n",
    "print('Var1: mean=%.3f stdv=%.3f' % (np.mean(Var1), np.std(Var1)))\n",
    "print('Var2: mean=%.3f stdv=%.3f' % (np.mean(Var2), np.std(Var2)))\n",
    "\n",
    "\n",
    "\n",
    "x = np.array(Var1).reshape((-1, 1))\n",
    "y = np.array(Var2)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n",
    "model = LinearRegression(fit_intercept=True).fit(x, y)\n",
    "r_sq = model.score(x, y)\n",
    "\n",
    "print('Coefficient of determination:', r_sq)\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope:', model.coef_)\n",
    "\n",
    "y_pred = model.predict(x)\n",
    "print('Predicted response on existing Vars:', y_pred,sep='\\n')\n",
    "\n",
    "x_new = np.arange(5).reshape((-1, 1))\n",
    "print('New Vars',x_new, sep='\\n')\n",
    "y_new = model.predict(x_new)\n",
    "print('Predicted response based on new Vars:',y_new,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Plotting MULTI-Variables and Linear Regression (Dont Delete)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_excel('https://www.dropbox.com/s/yuodmq2il8f728s/reg.xlsx?raw=1')\n",
    "pd.set_option('max_rows', 5) #limits row output\n",
    "print (df)\n",
    "plt.scatter(df['Var 1'], df['Var 2'])\n",
    "plt.scatter(df['Var 2'], df['Var 3'])\n",
    "\n",
    "'''\n",
    "Var1 = df.iloc[:,0]\n",
    "Var2 = df.iloc[:,1]\n",
    "Var3 = df.iloc[:,2]\n",
    "'''\n",
    "\n",
    "y = np.array(df['Var 1'])\n",
    "x1 = np.array(df['Var 2']).reshape((-1, 1))\n",
    "x2 = np.array(df['Var 3']).reshape((-1, 1))\n",
    "\n",
    "print('y: mean=%.3f stdv=%.3f' % (np.mean(y), np.std(y)))\n",
    "print('Var2: mean=%.3f stdv=%.3f' % (np.mean(x1), np.std(x1)))\n",
    "print('Var3: mean=%.3f stdv=%.3f' % (np.mean(x2), np.std(x2)))\n",
    "\n",
    "xall = np.concatenate(((x1, x2)),axis=1)\n",
    "              \n",
    "model = LinearRegression(fit_intercept=True).fit(xall, y)\n",
    "\n",
    "r_sq = model.score(xall, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "y_pred = model.predict(xall)\n",
    "print('Predicted response on existing Vars:', y_pred,sep='\\n')\n",
    "\n",
    "x_new = np.arange(10).reshape((-1, 2))\n",
    "print('New Vars',x_new, sep='\\n')\n",
    "y_new = model.predict(x_new)\n",
    "print('Predicted response based on new Vars:',y_new,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Advanced MULTI-Variables and Linear Regression (Dont Delete)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_excel('https://www.dropbox.com/s/yuodmq2il8f728s/reg.xlsx?raw=1')\n",
    "pd.set_option('max_rows', 5) #limits row output\n",
    "print (df)\n",
    "plt.scatter(df['Var 1'], df['Var 2'])\n",
    "plt.scatter(df['Var 2'], df['Var 3'])\n",
    "\n",
    "'''\n",
    "Var1 = df.iloc[:,0]\n",
    "Var2 = df.iloc[:,1]\n",
    "Var3 = df.iloc[:,2]\n",
    "'''\n",
    "\n",
    "y = np.array(df['Var 1'])\n",
    "x1 = np.array(df['Var 2']).reshape((-1, 1))\n",
    "x2 = np.array(df['Var 3']).reshape((-1, 1))\n",
    "\n",
    "print('y: mean=%.3f stdv=%.3f' % (np.mean(y), np.std(y)))\n",
    "print('Var2: mean=%.3f stdv=%.3f' % (np.mean(x1), np.std(x1)))\n",
    "print('Var3: mean=%.3f stdv=%.3f' % (np.mean(x2), np.std(x2)))\n",
    "\n",
    "xall = np.concatenate(((x1, x2)),axis=1)\n",
    "xall = sm.add_constant(xall) #REMOVE TO FORCE 0.0 R^2 will not be true\n",
    "\n",
    "model = sm.OLS(y, xall)\n",
    "results = model.fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Plotting Variables WITH LINE and Linear Regression (Dont Delete)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "df = pd.read_excel('https://www.dropbox.com/s/oqtubl4vetqf681/plot.xlsx?raw=1')\n",
    "pd.set_option('max_rows', 5) #limits row output\n",
    "print (df)\n",
    "\n",
    "\n",
    "Var1 = df.iloc[:,0]\n",
    "Var2 = df.iloc[:,1]\n",
    "\n",
    "\n",
    "print('Var1: mean=%.3f stdv=%.3f' % (np.mean(Var1), np.std(Var1)))\n",
    "print('Var2: mean=%.3f stdv=%.3f' % (np.mean(Var2), np.std(Var2)))\n",
    "\n",
    "\n",
    "\n",
    "x = np.array(Var1)\n",
    "y = np.array(Var2)\n",
    "\n",
    "# Fit with polyfit\n",
    "b, m = polyfit(x, y, 1)\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, b + m * x, '-')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression and Coefs (Dont Delete)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "df = pd.read_csv(\"https://www.dropbox.com/s/3go636sdiojcjw7/diabetes.csv?raw=1\", header=None, names=col_names)\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "\n",
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "\n",
    "x = df[feature_cols] # Features\n",
    "y = df.label\n",
    "\n",
    "# split x and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, \n",
    "            random_state=0)\n",
    "\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(x_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(x_test)\n",
    "\n",
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:',cnf_matrix,sep='\\n')\n",
    "y_test = [ int(s) for s in y_test]\n",
    "y_pred = [ int(s) for s in y_pred]\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "parameters = logreg.coef_\n",
    "intercept = logreg.intercept_\n",
    "np.set_printoptions(suppress=True)\n",
    "print('X Paras:',parameters,sep='\\n')\n",
    "print('Y Para:',intercept,sep='\\n')\n",
    "pd.set_option('max_rows', 7)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# DataFrame Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Manipulating Data Frames (Dont Delete)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"A\": [1,2,3], \"B\": [2,3,-4]}) #NOTE Curly brackets\n",
    "print('Original DF')\n",
    "print(df)\n",
    "\n",
    "#Selecting a column\n",
    "print('Printing of selected column')\n",
    "selecting_a = df['A']\n",
    "print(selecting_a)\n",
    "\n",
    "#Add particular cells together\n",
    "\n",
    "def adding(a):\n",
    "    return df.iloc[a,0] + df.iloc[a,1]\n",
    "\n",
    "print('')\n",
    "print('New Data Frame with sums of A and B')\n",
    "print(adding([0,1,2]))\n",
    "\n",
    "#Filtering dataframe for > values and useing absolute values\n",
    "print('')\n",
    "print('Doing a boolean test on Original DF abs() > 2')\n",
    "print(df.abs() > 2)\n",
    "print('Applying boolean test to DF')\n",
    "print(df[df.abs() > 2])\n",
    "\n",
    "# Extending a Lists then making a new DataFrame\n",
    "print('')\n",
    "print('Initial List')\n",
    "list1 = [3,5,7]\n",
    "print(list1)\n",
    "print('Extended List')\n",
    "list1.extend([2,7,1])\n",
    "print(list1)\n",
    "print('Extended List to DF')\n",
    "df1 = pd.DataFrame({'C': list1})\n",
    "print(df1)\n",
    "\n",
    "# Adding 1 DF to another. Adding previous DF from string\n",
    "print('')\n",
    "print('Merged Data Frame Horizontally')\n",
    "dfnew = pd.concat([df, df1], axis=1) #Axis = 1 for horizontal\n",
    "print(dfnew)\n",
    "df2 = pd.DataFrame({\"X\": [1,2,3], \"Y\": [4,5,6], \"Z\": [7,8,9]}) #NOTE Curly brackets\n",
    "print('New Data Frame to add vertically')\n",
    "print(df2)\n",
    "print('Merged Data Frame Vertically')\n",
    "df2.columns = dfnew.columns #Rename columns to DF columns to merge with\n",
    "result = pd.concat([dfnew, df2], axis=0) #Axis = 0 for horizontal\n",
    "result.reset_index(inplace = True, drop = True) #Resets index. Make sure inplace = True to replace original DataFrame and drop = True to drop old index\n",
    "print(result)\n",
    "\n",
    "#Finding and replacing items in data frame\n",
    "print('')\n",
    "dffandr = pd.DataFrame({\"A\": [1,2,3], \"B\": [2,3,-4], \"D\": [3,2,-4]})\n",
    "print('DF to find and reaplce')\n",
    "print(dffandr)\n",
    "dffandrnew = dffandr.replace([1,2,3,-4], ['Duck', 'Cat', 'Dog', 'Lion'])\n",
    "print(dffandrnew)\n",
    "\n",
    "#Selecting a column\n",
    "print('Printing of selected columns A and D')\n",
    "selecting_ad = dffandrnew[['A', 'D']] #Dont forget double brackets if more than 1 column\n",
    "print(selecting_ad)\n",
    "\n",
    "# Filtering out NaN's\n",
    "print('')\n",
    "print(\"Filtering out NaN's (removed last row of NaN's)\")\n",
    "filter2 = dffandr[(dffandr == 2) | (dffandr == 1)].dropna(how='all') # dropna ~ how=all drops rows that contain all NaN. how=any drops any NaN ALSO \"|\" = or \"&\" = and\n",
    "print(filter2)\n",
    "\n",
    "# Adding a new column\n",
    "print('')\n",
    "print('Adding a new column (E) as a function of (A+B)*2')\n",
    "\n",
    "filter2['E'] = (filter2.A + filter2.B)*2\n",
    "print(filter2)\n",
    "\n",
    "#Conditional statements on a column\n",
    "print('')\n",
    "print('Conditional statements on a column')\n",
    "pd.set_option('display.max_rows', 30)\n",
    "print(result)\n",
    "\n",
    "def mycolfunction(x):\n",
    "    if x > 5:\n",
    "        return 'Yay'\n",
    "    else:\n",
    "        return 'Na'\n",
    "    \n",
    "result['Fx(C)'] = result.C.apply(mycolfunction)\n",
    "\n",
    "print(result)\n",
    "\n",
    "#Conditional statements on a row\n",
    "print('')\n",
    "print('Conditional statements on a row. See what Fx(ABC) equals in code')\n",
    "\n",
    "result.iloc[4, result.columns.get_loc('B')] = 5 #replacing a Nan value with 5\n",
    "\n",
    "def myrowfunction (y):\n",
    "    if (pd.isnull(y.A) and pd.isnull(y.B)) == True:\n",
    "        return 5\n",
    "    elif ((y.A + y.B + y.C) * 2) > 15:\n",
    "        return ((y.A + y.B + y.C) * 2) + 100\n",
    "    else:\n",
    "        return (y.A + y.B + y.C) * 2\n",
    "    \n",
    "result['Fx(ABC)'] = result.apply(myrowfunction, axis = 1) #dont forget axis = 1 to specify calc in rows\n",
    "\n",
    "print(result)\n",
    "\n",
    "#Renaming columns. Using previou DF\n",
    "print('')\n",
    "print('Renaming columns. Using previous DF')\n",
    "\n",
    "#Changing ALL columns\n",
    "result.columns = ['X', 'Y', 'Z', 'Fx(C)', 'Fx(ABC)']\n",
    "\n",
    "#Changing select columns\n",
    "result.rename(columns={'Fx(C)': 'Fx(Z)', 'Fx(ABC)': 'Fx(XYZ)'}, inplace=True)\n",
    "\n",
    "print(result)\n",
    "\n",
    "#Checking if column has unique questions\n",
    "print('')\n",
    "Utest = result['X'].is_unique\n",
    "print('Is the data unique?', Utest)\n",
    "\n",
    "#summarizing-counts-of-values-for-multiple-cols\n",
    "print('')\n",
    "print('Summarizing-counts-of-values-for-multiple-cols')\n",
    "\n",
    "countsdf = pd.DataFrame({'X': ['Agree', 'Disagree', 'Agree', 'Neutral', 'Agree','Neutral'],\n",
    "               'Y': ['Disagree', 'Neutral', 'Agree', 'Disagree', 'Agree', 'Neutral'], \n",
    "               'Z': ['Agree', 'Neutral', 'Neutral', 'Disagree', 'Neutral','Neutral']})\n",
    "print('DF to count')\n",
    "print(countsdf)\n",
    "\n",
    "print('Counts on the DF Complex Way')\n",
    "out = pd.DataFrame()\n",
    "for col in countsdf.columns:\n",
    "    out = out.append(countsdf[col].value_counts())\n",
    "\n",
    "out = out.transpose()\n",
    "print(out)\n",
    "print('Counts on the DF Easy Way')\n",
    "\n",
    "out_easy=countsdf.apply(pd.value_counts)\n",
    "print(out_easy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame and populating it with a loop (DONT DELETE)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "numberofrows = 10 #This number controls the rows\n",
    "index = pd.Series(range(0,numberofrows))\n",
    "columns = ['x']\n",
    "\n",
    "dfoutput = pd.DataFrame(index=index, columns=columns)\n",
    "print('Row Size:')\n",
    "print(dfoutput.shape)\n",
    "print('')\n",
    "\n",
    "for n in range(0, numberofrows):\n",
    "    x = n*2\n",
    "    y = x**2\n",
    "    dfoutput.loc[n] = x\n",
    "    dfoutput.loc[[n],'y'] = y\n",
    "\n",
    "print(dfoutput)\n",
    "print('')\n",
    "print('Output Size:')\n",
    "print(dfoutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview of data to apply aggregate functions\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'ordersfordfexc.csv' does not exist: b'ordersfordfexc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-629f4d1a65c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Preview of data to apply aggregate functions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdfagg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ordersfordfexc.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#index_col= is great for setting custom index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfagg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'ordersfordfexc.csv' does not exist: b'ordersfordfexc.csv'"
     ]
    }
   ],
   "source": [
    "# Calculating Aggregate Functions PLEASE LOAD ordersfordfexc.csv\n",
    "print('')\n",
    "print('Preview of data to apply aggregate functions')\n",
    "dfagg = pd.read_csv('ordersfordfexc.csv', index_col='id') #index_col= is great for setting custom index\n",
    "\n",
    "print(dfagg.head())\n",
    "# df.groupby('column1').column2.measurement()\n",
    "pricey_shoes = dfagg.groupby('shoe_type').price.max().reset_index()\n",
    "\n",
    "print('')\n",
    "print('Max price of shoes')\n",
    "print(pricey_shoes)\n",
    "\n",
    "def percentile25 (z):\n",
    "    return np.percentile(z, 25)\n",
    "\n",
    "cheap_shoes = dfagg.groupby('shoe_color').price.apply(percentile25).reset_index()\n",
    "print('')\n",
    "print('Lowest 25th percentile')\n",
    "print(cheap_shoes.head())\n",
    "\n",
    "shoe_counts = dfagg.groupby(['shoe_type', 'shoe_color']).size().reset_index(name='count')\n",
    "print('')\n",
    "print('Count of shoes')\n",
    "print(shoe_counts)\n",
    "\n",
    "pivoted = shoe_counts.pivot(columns='shoe_color', index='shoe_type', values='count').reset_index(drop=False)\n",
    "print('')\n",
    "print('Pivot Table')\n",
    "print(pivoted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Q1     Q2     Q3\n",
      "0    A,B    B,A    C,A\n",
      "1    A,C    C,A    C,B\n",
      "2    A,B  B,C,A    A,B\n",
      "3    B,C    A,B    C,B\n",
      "4  A,B,C    A,C  A,B,C\n",
      "5  C,B,A    B,C  A,B,C\n",
      "6  B,C,A    C,B    C,A\n",
      "   Q1_A  Q1_B  Q1_C  Q2_A  Q2_B  Q2_C  Q3_A  Q3_B  Q3_C\n",
      "0     1     1     0     1     1     0     1     0     1\n",
      "1     1     0     1     1     0     1     0     1     1\n",
      "2     1     1     0     1     1     1     1     1     0\n",
      "3     0     1     1     1     1     0     0     1     1\n",
      "4     1     1     1     1     0     1     1     1     1\n",
      "5     1     1     1     0     1     1     1     1     1\n",
      "6     1     1     1     0     1     1     1     0     1\n"
     ]
    }
   ],
   "source": [
    "# 0 1 catagory matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Q1': ['A,B', 'A,C', 'A,B', 'B,C', 'A,B,C','C,B,A','B,C,A'],\n",
    "               'Q2': ['B,A', 'C,A', 'B,C,A', 'A,B', 'A,C', 'B,C','C,B'], \n",
    "               'Q3': ['C,A', 'C,B', 'A,B', 'C,B', 'A,B,C','A,B,C','C,A']})\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "df = pd.concat([df[x].str.get_dummies(',') for x in df], axis = 1, keys=df.columns) # Creates 0 1 matrices the join them together\n",
    "df.columns = map('_'.join, df.columns) #joins each question to its topic with an '_'\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survey ID  State_NSW  State_VIC  State_ACT  Food_Pizza  Food_Chips  \\\n",
      "0          1          1          1          1           0           0   \n",
      "1          2          1          0          0           0           1   \n",
      "2          3          1          1          0           1           0   \n",
      "\n",
      "   Food_Cake  Gender Asian  \n",
      "0          0    Male   Yes  \n",
      "1          0  Female   Yes  \n",
      "2          1    Male    No  \n",
      "   Survey ID  Gender Asian   Food State\n",
      "0          1    Male   Yes    NaN   ACT\n",
      "1          1    Male   Yes    NaN   NSW\n",
      "2          1    Male   Yes    NaN   VIC\n",
      "3          2  Female   Yes  Chips   NSW\n",
      "4          3    Male    No   Cake   NSW\n",
      "5          3    Male    No  Pizza   VIC\n"
     ]
    }
   ],
   "source": [
    "# Pivoting 0 1 catagory matrix\n",
    "\n",
    "df = pd.DataFrame({'Survey ID': [1,2,3],\n",
    "                   'State_NSW': [1,1,1], \n",
    "                   'State_VIC': [1,0,1], \n",
    "                   'State_ACT': [1,0,0],\n",
    "                   'Food_Pizza': [0,0,1], \n",
    "                   'Food_Chips': [0,1,0], \n",
    "                   'Food_Cake': [0,0,1],\n",
    "                   'Gender': ['Male', 'Female', 'Male'],\n",
    "                   'Asian': ['Yes', 'Yes', 'No']})\n",
    "                  \n",
    "\n",
    "cols = ['Survey ID',  'Gender', 'Asian'] # SET ALL COLUMNS NOT NEEDING TRANSFORMATION HERE\n",
    "colslen = len(cols)+1\n",
    "\n",
    "#convert to MultiIndex all not Q topic columns\n",
    "df2 = df.set_index(cols)\n",
    "\n",
    "#split columns names to MultiIndex in columns\n",
    "df2.columns = df2.columns.str.split(pat = '_', expand=True) # pat = CHOOSE SEPERATOR\n",
    "\n",
    "#reshape\n",
    "df2 = df2.stack(level=-1, dropna=True)\n",
    "\n",
    "#filter only rows with at least one 1 per row and reshape for remove NaNs\n",
    "filt = df2.eq(1) #defining filter to be true if 1 and false for the rest \n",
    "filt = filt.any(axis = 1) #modifying filter to show true if ANY value is true for all columns. In other words show false if all columns are false (not 1) \n",
    "df2 = df2[filt] #apply true/false filter to df2\n",
    "df2 = df2.replace(0, np.nan) #replaces 0 with Nan\n",
    "df2 = df2.stack(dropna=True) #pivots and drops all Nan's\n",
    "df2 = df2.reset_index(level=-2) #resets index to level NEGATIVE-2 \n",
    "\n",
    "#added helper level to MultiIndex because possible duplicates by counter\n",
    "rng = list(range(colslen))\n",
    "df2['g'] = df2.groupby(level=rng).cumcount() #this adds another column g that adds to the cumulation given ALL index levels\n",
    "\n",
    "#final reshape\n",
    "df2 = df2.set_index('g', append=True) #add new g column as index. this is an append so it does not delete the other indexs\n",
    "df2 = df2[df2.columns[0]] # this only selects the first column as the other is just 1's (not very useful)\n",
    "df2 = df2.unstack(-2) # pivots the index base on level. In this case level NEGATIVE -2\n",
    "df2 = df2.reset_index(level=-1, drop=True)\n",
    "df2 = df2.reset_index()\n",
    "print(df)\n",
    "print(df2)\n",
    "\n",
    "df2.to_excel ('df2.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Control Flow IF ELIF and ELSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Example of control flow - finding the cheapest shipping method (Do not Delete)\n",
    "\n",
    "def shipping_cost_ground(weight): #Useful for defining a function\n",
    "    if weight <= 2:\n",
    "        price_per_pound = 1.50\n",
    "    elif weight <= 6:\n",
    "        price_per_pound = 3\n",
    "    elif weight <= 10:\n",
    "        price_per_pound = 4\n",
    "    else:\n",
    "        price_per_pound = 4.75\n",
    "        \n",
    "    return 20 + (price_per_pound * weight)\n",
    "\n",
    "shipping_cost_premium = 125\n",
    "\n",
    "def shipping_cost_drone(weight):\n",
    "    if weight <= 2:\n",
    "        price_per_pound = 4.5\n",
    "    elif weight <= 6:\n",
    "        price_per_pound = 9\n",
    "    elif weight <= 10:\n",
    "        price_per_pound = 12\n",
    "    else:\n",
    "        price_per_pound = 14.25\n",
    "        \n",
    "    return price_per_pound * weight\n",
    "\n",
    "def print_cheapest_shipping_method(weight):\n",
    "    \n",
    "    ground = shipping_cost_ground(weight)\n",
    "    premium = shipping_cost_premium\n",
    "    drone = shipping_cost_drone(weight)\n",
    "    \n",
    "    if ground < premium and ground < drone:\n",
    "        method = 'Standard Ground'\n",
    "        cost = ground\n",
    "    elif premium < ground and premium < drone:\n",
    "        method = 'Premium Ground'\n",
    "        cost = premium\n",
    "    else:\n",
    "        method = 'Drone'\n",
    "        cost = drone\n",
    "    \n",
    "    print('The cheapest option available is $%.2f with %s shipping.' %(cost,method)) # % will be replaced with defined variables\n",
    "\n",
    "for n in range(0,30):\n",
    "    print_cheapest_shipping_method(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Importing data from MySQL\n",
    "#*** Need to !pip install mysql-connector-python ***\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:amish@35.201.7.91:3306/amish', echo=True)\n",
    "\n",
    "query = \"select * from amish.orders\"\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Writing to a table in MySQL\n",
    "#*** Need to !pip install mysql-connector-python ***\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:amish@35.201.7.91:3306/amish', echo=True) #create_engine('mysql+mysqlconnector://user:pwd@hostname/db_name')\n",
    "\n",
    "df2 = pd.DataFrame({\"a\": [7,7,6], \"b\": [3,3,4], \"c\": [2,2,2]})\n",
    "\n",
    "df2.to_sql(name='test',con=engine,if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Text Analysis\n",
    "\n",
    "! pip install -U textblob\n",
    "! python -m textblob.download_corpora\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_excel('united.xlsx')\n",
    "df = df.iloc[:,0].replace({'-':'','/':'','  ':' '}, regex=True)\n",
    "df = pd.DataFrame(df)\n",
    "df = df.dropna()\n",
    "\n",
    "df['processed'] = df.iloc[:,0].apply(lambda x: \" \".join(x.lower() for x in x.split())) # Apply all lower case\n",
    "df['processed'] = df['processed'].str.replace('[^\\w\\s]','') #Removing Punctuation\n",
    "df['word_count'] = df.iloc[:,0].apply(lambda x: len(str(x).split(\" \"))) # Word count\n",
    "df['char_count'] = df.iloc[:,0].str.len() # Character count\n",
    "df['sentiment'] = df.iloc[:,0].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df['obj/subj'] = df.iloc[:,0].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "\n",
    "def avg_word(sentance):\n",
    "    words = sentance.split()\n",
    "    return (sum(len(letters) for letters in words))/len(words)\n",
    "\n",
    "#df['avg_word_len'] = df.iloc[:,0].apply(lambda x: float(\"{0:.2f}\".format(avg_word(x)))) ACTIVATE ONLY IF NOT DIV by 0\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + ['Amish', 'Haria'] # Add custom stopwords here\n",
    "\n",
    "df['stop_words'] = df['processed'].apply(lambda x: len([x for x in x.split() if x in stop])) # Number of stop words in text\n",
    "\n",
    "df['processed'] = df['processed'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) # Removal of stop words\n",
    "df['processed'] = df['processed'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) #Lemmatization\n",
    "\n",
    "#Unigram Count\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 1))\n",
    "cv_fit=cv.fit_transform(df['processed'])\n",
    "\n",
    "vector = cv_fit.toarray().sum(axis=0)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "unigram = pd.DataFrame(vector.T, index=feature_names, columns=[\"Count\"])\n",
    "unigram = unigram.sort_values(by=[\"Count\"],ascending=False)\n",
    "unigram = unigram.head(35)\n",
    "print(unigram)\n",
    "\n",
    "#Bigram Count\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "cv_fit=cv.fit_transform(df['processed'])\n",
    "\n",
    "vector = cv_fit.toarray().sum(axis=0)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "bigram = pd.DataFrame(vector.T, index=feature_names, columns=[\"Count\"])\n",
    "bigram = bigram.sort_values(by=[\"Count\"],ascending=False)\n",
    "bigram = bigram.head(25)\n",
    "print(bigram)\n",
    "\n",
    "#Trigram Count\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "cv_fit=cv.fit_transform(df['processed'])\n",
    "\n",
    "vector = cv_fit.toarray().sum(axis=0)\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "trigram = pd.DataFrame(vector.T, index=feature_names, columns=[\"Count\"])\n",
    "trigram = trigram.sort_values(by=[\"Count\"],ascending=False)\n",
    "trigram = trigram.head(15)\n",
    "print(trigram)\n",
    "\n",
    "#Unigram TFIDF\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vectorizer_fit=tfidf_vectorizer.fit_transform(df['processed'])\n",
    "\n",
    "vector = tfidf_vectorizer_fit.toarray().sum(axis=0)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "unigramtfidf = pd.DataFrame(vector.T, index=feature_names, columns=[\"Score\"])\n",
    "unigramtfidf = unigramtfidf.sort_values(by=[\"Score\"],ascending=False)\n",
    "unigramtfidf = unigramtfidf.head(35)\n",
    "print(unigramtfidf)\n",
    "\n",
    "#Bigram TFIDF\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(ngram_range=(2, 2))\n",
    "tfidf_vectorizer_fit=tfidf_vectorizer.fit_transform(df['processed'])\n",
    "\n",
    "vector = tfidf_vectorizer_fit.toarray().sum(axis=0)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "bigramtfidf = pd.DataFrame(vector.T, index=feature_names, columns=[\"Score\"])\n",
    "bigramtfidf = bigramtfidf.sort_values(by=[\"Score\"],ascending=False)\n",
    "bigramtfidf = bigramtfidf.head(25)\n",
    "print(bigramtfidf)\n",
    "\n",
    "#Trigram TFIDF\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(ngram_range=(3, 3))\n",
    "tfidf_vectorizer_fit=tfidf_vectorizer.fit_transform(df['processed'])\n",
    "\n",
    "vector = tfidf_vectorizer_fit.toarray().sum(axis=0)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "trigramtfidf = pd.DataFrame(vector.T, index=feature_names, columns=[\"Score\"])\n",
    "trigramtfidf = trigramtfidf.sort_values(by=[\"Score\"],ascending=False)\n",
    "trigramtfidf = trigramtfidf.head(15)\n",
    "print(trigramtfidf)\n",
    "\n",
    "print(df)\n",
    "\n",
    "with pd.ExcelWriter('Qual_Analysis.xlsx') as writer:\n",
    "    unigramtfidf.to_excel(writer, sheet_name='Unigram TFIDF')\n",
    "    bigramtfidf.to_excel(writer, sheet_name='Bigram TFIDF')\n",
    "    trigramtfidf.to_excel(writer, sheet_name='Trigram TFIDF')\n",
    "    unigram.to_excel(writer, sheet_name='Unigram Freq')\n",
    "    bigram.to_excel(writer, sheet_name='Bigram Freq')\n",
    "    trigram.to_excel(writer, sheet_name='Trigram Freq')\n",
    "    df.to_excel(writer, sheet_name='Stats')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Qualtrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Qualtrics Excel Cleanup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel('Q1 2019.xlsx')\n",
    "\n",
    "column_names_df = df.iloc[0:1,:]\n",
    "column_names_df = column_names_df.values.tolist()\n",
    "df.columns = column_names_df[0]\n",
    "df = df.drop(df.index[0])\n",
    "df.head()\n",
    "df.to_excel ('export_dataframe.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":{\"progressId\":\"ES_0Hzrd1dMs9rwLHL\",\"percentComplete\":0.0,\"status\":\"inProgress\"},\"meta\":{\"requestId\":\"90a8c9fa-9cb0-4c7c-8026-6b9e7477db0f\",\"httpStatus\":\"200 - OK\"}}\n",
      "progressStatus= inProgress\n",
      "Download is 0.0 complete\n",
      "progressStatus= inProgress\n",
      "Download is 50.76142131979695 complete\n",
      "progressStatus= inProgress\n",
      "Download is 100.0 complete\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Qualtrics download through API \n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "import io, os\n",
    "import sys\n",
    "\n",
    "# Setting user Parameters\n",
    "\n",
    "\n",
    "apiToken = \"1zu3pwieQFYHXVPUjm2py3N1ufgaG5E0fwWfdv3W\"\n",
    "\n",
    "surveyId = \"SV_8vKAWaBM2wr0Tk1\"\n",
    "\n",
    " \n",
    "\n",
    "fileFormat = \"csv\"\n",
    "\n",
    "dataCenter = 'au1'\n",
    "\n",
    " \n",
    "\n",
    "# Setting static parameters\n",
    "\n",
    "requestCheckProgress = 0.0\n",
    "\n",
    "progressStatus = \"inProgress\"\n",
    "\n",
    "baseUrl = \"https://{0}.qualtrics.com/API/v3/surveys/{1}/export-responses/\".format(dataCenter, surveyId)\n",
    "\n",
    "headers = {\n",
    "\n",
    "    \"content-type\": \"application/json\",\n",
    "\n",
    "    \"x-api-token\": apiToken,\n",
    "\n",
    "    }\n",
    "\n",
    " \n",
    "\n",
    "# Step 1: Creating Data Export\n",
    "\n",
    "downloadRequestUrl = baseUrl\n",
    "\n",
    "downloadRequestPayload = '{\"format\":\"' + fileFormat + '\"}'\n",
    "\n",
    "downloadRequestResponse = requests.request(\"POST\", downloadRequestUrl, data=downloadRequestPayload, headers=headers)\n",
    "\n",
    "progressId = downloadRequestResponse.json()[\"result\"][\"progressId\"]\n",
    "\n",
    "print(downloadRequestResponse.text)\n",
    "\n",
    " \n",
    "\n",
    "# Step 2: Checking on Data Export Progress and waiting until export is ready\n",
    "\n",
    "while progressStatus != \"complete\" and progressStatus != \"failed\":\n",
    "\n",
    "    print (\"progressStatus=\", progressStatus)\n",
    "\n",
    "    requestCheckUrl = baseUrl + progressId\n",
    "\n",
    "    requestCheckResponse = requests.request(\"GET\", requestCheckUrl, headers=headers)\n",
    "\n",
    "    requestCheckProgress = requestCheckResponse.json()[\"result\"][\"percentComplete\"]\n",
    "\n",
    "    print(\"Download is \" + str(requestCheckProgress) + \" complete\")\n",
    "\n",
    "    progressStatus = requestCheckResponse.json()[\"result\"][\"status\"]\n",
    "\n",
    " \n",
    "\n",
    "# step 2.1: Check for error\n",
    "\n",
    "if progressStatus is \"failed\":\n",
    "\n",
    "    raise Exception(\"export failed\")\n",
    "\n",
    " \n",
    "\n",
    "fileId = requestCheckResponse.json()[\"result\"][\"fileId\"]\n",
    "\n",
    " \n",
    "\n",
    "# Step 3: Downloading file\n",
    "\n",
    "requestDownloadUrl = baseUrl + fileId + '/file'\n",
    "\n",
    "requestDownload = requests.request(\"GET\", requestDownloadUrl, headers=headers, stream=True)\n",
    "\n",
    " \n",
    "\n",
    "# Step 4: Unzipping the file\n",
    "\n",
    "zipfile.ZipFile(io.BytesIO(requestDownload.content)).extractall(\"MyQualtricsDownload\")\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option('max_rows', 50)\n",
    "\n",
    "df = pd.read_excel(\"https://www.dropbox.com/s/wjfar4io9jxnz5n/cryotherapy.xlsx?raw=1\")\n",
    "\n",
    "df.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "x = df[['sex', 'age', 'Number_of_Warts', 'Type', 'Area']]\n",
    "y = df['Result_of_Treatment']\n",
    "\n",
    "# split x and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.31, random_state=0)\n",
    "\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(x_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(x_test)\n",
    "\n",
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:',cnf_matrix,sep='\\n')\n",
    "y_test = [ int(s) for s in y_test]\n",
    "y_pred = [ int(s) for s in y_pred]\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "parameters = logreg.coef_\n",
    "intercept = logreg.intercept_\n",
    "\n",
    "print('X Paras:',parameters,sep='\\n')\n",
    "print('Y Para:',intercept,sep='\\n')\n",
    "pd.set_option('max_rows', 7)\n",
    "print(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Exploratory Factor Analysis (Do Not Delete) n = 100 is poor, 200 is fair, 300 is good, 500 is very good, and 1000 or more is excellent\n",
    "# ***Install package first. ! pip install factor_analyzer***\n",
    "\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.datasets import load_iris\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"https://www.dropbox.com/s/110tmphef00ybyg/bfi.csv?raw=1\")\n",
    "\n",
    "#show existing columns\n",
    "print(df.columns) \n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df.drop(['gender', 'education', 'age', 'Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "#Dropping missing values rows\n",
    "df.dropna(inplace=True) # Removes Entire row that has NA\n",
    "\n",
    "# Summary of data\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "#ADEQUACY TEST (Testing factorability)\n",
    "\n",
    "print('')\n",
    "\n",
    "#Bartletts test\n",
    "\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
    "print('Bartletts test:',chi_square_value, p_value,sep='\\n')\n",
    "\n",
    "#Kaiser-Meyer-Olkin (KMO) Test\n",
    "\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df)\n",
    "print('Kaiser-Meyer-Olkin (KMO) Test(>0.8):', kmo_model,sep='\\n')\n",
    "\n",
    "#CHOOSING NUMBER OF FACTORS\n",
    "\n",
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(25, rotation=None)\n",
    "\n",
    "print('')\n",
    "\n",
    "# Check Eigenvalues\n",
    "fa.fit(df)\n",
    "\n",
    "ev, v = fa.get_eigenvalues()\n",
    "\n",
    "print('Eigen Values Test(>1.0):',ev[ev > 0.95],sep=\"\\n\")\n",
    "\n",
    "# Create scree plot using matplotlib\n",
    "plt.scatter(range(1,df.shape[1]+1),ev)\n",
    "plt.plot(range(1,df.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Create factor analysis object and perform factor analysis\n",
    "\n",
    "fa1 = FactorAnalyzer(5, rotation=\"varimax\") #Change number of factors HERE\n",
    "fa1.fit(df)\n",
    "\n",
    "loadings = pd.DataFrame(fa1.loadings_)\n",
    "loadingsabs = loadings.abs()\n",
    "\n",
    "print(loadingsabs)\n",
    "\n",
    "filtered = loadingsabs[(loadingsabs > 0.35)]\n",
    "\n",
    "print(filtered)\n",
    "\n",
    "# Get variance of each factors\n",
    "\n",
    "ss, pv, cv = fa1.get_factor_variance()\n",
    "\n",
    "print('')\n",
    "print('SS Loadings:',ss,sep=\"\\n\")\n",
    "print('Proportion Var:',pv,sep=\"\\n\")\n",
    "print('Cumulative Var:',cv,sep=\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#PCA (Dont Delete)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "genes = ['gene' + str(i) for i in range(1,101)]\n",
    "\n",
    "wt = ['wt' + str(i) for i in range(1,6)]\n",
    "ko = ['ko' + str(i) for i in range(1,6)]\n",
    "\n",
    "data = pd.DataFrame(columns = [*wt, *ko], index = genes)\n",
    "\n",
    "for gene in data.index:\n",
    "    data.loc[gene, 'wt1':'wt5'] = np.random.poisson(lam=rd.randrange(10,1000), size = 5)\n",
    "    data.loc[gene, 'ko1':'ko5'] = np.random.poisson(lam=rd.randrange(10,1000), size = 5)\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "#Centers the data so mean will be 0 and StDev will be 1. T will transpose becuase scale function expects samples to be in rows not columns.\n",
    "scaled_data = preprocessing.scale(data.T) \n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_data = pca.transform(scaled_data)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "pca_df = pd.DataFrame(pca_data, index=[*wt, *ko], columns=labels)\n",
    "\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title('My PCA Graph')\n",
    "plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "plt.ylabel('PC2 - {0}%'.format(per_var[1]))\n",
    " \n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# get the name of the top 20 measurements (genes) that contribute\n",
    "## most to pc1.\n",
    "## first, get the loading scores\n",
    "loading_scores = pd.Series(pca.components_[0], index=genes)\n",
    "## now sort the loading scores based on their magnitude\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    " \n",
    "# get the names of the top 10 genes\n",
    "top_20_genes = sorted_loading_scores[0:20].index.values\n",
    " \n",
    "## print the gene names and their scores (and +/- sign)\n",
    "print(loading_scores[top_20_genes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
